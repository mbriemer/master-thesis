%\section{Method}
%\label{sec:method}

\section{Background} %TODO make section?
\label{sec:background}

\subsection{Structural estimation}
\label{sec:structural_estimation}

Consider the problem of estimating the parameters of a structural economic model.
%TODO Is this a good framing?
%For $k \in \{1,\dots,K\}$ and $l \in \{1,\dots,L\}$, 
Let $Z \sim p(Z)$ be a vector of ranodm noise variables, $\Theta$ a parameter space and $\mathcal{X}$ a space of outcomes or observations.
Then an economic model can be imagined as a function $f$ that maps a draw from the noise $Z$ and a set of parameters $\theta \in \Theta$ to an outcome $X \in \mathcal{X}$:

\begin{equation}
    X = f(Z, \theta).
\end{equation}

$f$ might be quite complicated, in particular and as suggested by the index notation, a system of equations. %TODO k, l
What makes the estimation \textit{structural} is that $f$ is an implementation of a particular economic model together with the assumption that this model captures the true relationships between economic variables, rather than just their statistical relationships. %TODO maybe rewrite
%where $X_k$ is a vector of outcome variables influenced by a vector of noise variables $Z_l$. %via a set of (possibly endogeneous) variables $X_k$.
%The strength and functional form of the relationships between the variables is defined by a function $f$ and its parameters $\Theta$.

$Z \sim p(Z)$ induces a distribution $p(X_{1, \dots, n}|\theta)$.
This suggests the standard approach of maximum likelihood estimation, that is, to find $\hat{\theta}$ such that

\begin{equation}
    \hat{\theta}_{MLE} = \underset{\theta\in\Theta}{\operatorname{arg\;max}}\,\text{log} p(X_{1, \dots, n}|\theta).
\end{equation}

However, for some more sophisticated economic models, it is not easy or even possible to give an analytical expression for the likelihood function.

This motivates further approaches, in particular simulation methods, which attempt to infer $\theta$ based on a simulation of the true data.
Most notable among these is perhaps the simulated method of moments, which performs inference based on the moments of the simulated data. %TODO verify
%TODO more on problems of SMM?
%TODO indirect inference?

The question naturally arises of how to judge whether the simulated distribution comes sufficiently close to the real distribution.
This is motivates the idea of introducing a second component that provides ``feedback'' on this question in the form of a criterion function to be minimzed.
Updating the estimate $\hat{theta}$ to minimize this criterion is the central idea behind adversarial estimation, which I will discuss in more detail in section \ref{sec:adversarial_estimation}.

One idea for defining the criterion is to base it on a classification, more precisely, on the probability that a given data point was drawn from the real data rather from a simulated distribution.
In machine learning, a popular tool for classification are neural networks, which I introduce next.

\subsection{Neural networks}
\label{sec:neural_networks}

A neural network, in the most general sense, is a directed graph that defines how a certain output should be calculated from a given input.
There are many different structures (also called \textit{architectures}) of that graph.
This thesis only considers so-called \textit{feed-forward neural networs}, for which the graph consists of:

\begin{itemize}
    \item An input layer with one node for each input variable
    \item An ordered set of hidden layers. The number of nodes in each is chosen by the researcher. The nodes are connected by incoming edges only the the previous and by outgoing edges only to the succesive layer
    \item An output layer with one node for each output variable
\end{itemize}

Embedded in any architecture is the fundamental notion of neural networks, which imagines nodes as neurons that calculate inputs and outputs according to their edges.

\begin{definition}[Feed-forward neural network]
    Let $l = 1, \dots, L$ be an ordered set of layers, with 1 the input and $L$ the output layer.
    Let $I_l$ be the number of nodes in each layer with $b_{l,1} \dots, b_{l,I_l}$ and $\sigma_{l,1} \dots, \sigma_{l,I_l}$ their biases and activation functions, respectively.
    Let $J_l$ the number of incoming edges of each layer $l = 2, \cdots, L$, and let each node have at least one 
    %More precisely, each edge is associated with a weight $w_{l,j}$ and each node in the hidden or ouput layers $v_{l,i}$ with a constant $b_{l,i}$ (called \textit{bias}) and a sigmoid \textit{activation function} $\sigma_{l,i}$.
    For the nodes in the input layer $v_{1,i}$, set their output equal the inputs.
    For the nodes $v_{l,i}, l \geq 2$, set:

    \begin{equation}
        v_{l,i} = \sigma (\sum_{j = 1}^{J_l} w_{l,J} \cdot v_{(l-1),i})
    \end{equation}

    and interpret the output of the last nodes $v_{L,i}$ as the output of the network.

    Then $\mathcal{N} = (V, E, \Psi)$ is called a neural network
    with nodes $V$, edges $E$ and $\Psi$ holds the weights, biases and activation functions.% after training according to $\theta_{train}$.
\end{definition}

Note that if the last layer consist only of one node taking values in $[0,1]$ the network can be interpreted as a classifier suitable to the real-or-fake data problem introduced above.

In practice, neural networks usually used for machine learning applications and their paramters \textit{trained} using a gradient descent or other optimization method.
In particular, they are sometimes not trained until the optimizer converges, so it is reasonable to include a specification of the training process in the definition.

\begin{definition}[Trained feed-forwared neural network]
    Let $\theta_{train}$ be a set of hyperparameters specifying the training of a neural network $\mathcal{N}$, inlcuding at least:
    \begin{itemize}
        \item A set of initial parameters $\Psi_0$
        \item A training algorithm A
        \item A stopping criterion
    \end{itemize}
    Then call  $\mathcal{N}(\theta_{train}) = (V, E, \Psi(\theta_{train}))$ a feed-forward neural network trained according to $\theta_{train}$.
\end{definition}

In practice, $\Psi_0$ is usually chosen randomly
Note that the training algorithm A might itself require the setting of further hyperparameters.
So-called \textit{stochastic gradient descent} are the most common training methods, of which Adam (\cite{diederik2014adam}) is the most popular choice. %TODO elaborate?
The stopping criterion might be either the convergence of the training algorithm, or the a certain number of training steps having been performed.

\section{Adversarial estimation}
\label{sec:adversarial_estimation}

The basic idea of adversarial estimation is to structure the parameter estimation around two auxiliary models, called the \textit{generator} and the \textit{discriminator} (or \textit{critic}).
The generator $G : \Theta \times \mathcal{Z} \rightarrow \mathcal{X}$ creates simulated data based on a guess of the true parameter value $\hat{\theta} \in \Theta$.
Given the real and the simulated data, the discriminator returns objective function for the generator, which I will call \textit{loss}.
This loss function can be any divergence or distance between the distributions of the real and simulated data, inlcuding functions that are directy analytically tractable.
In the classic GAN case involving neural networks, the loss function is the result of the discriminator solving a maximization problem:

\begin{equation}
\label{eq:adversarial_estimator}
    \hat{\theta}_{adv} = \underset{\theta \in \Theta}{\arg \min } \max _{D \in \mathcal{D}} \text{loss}(D(X_i, G(\theta, Z))).
\end{equation}

Note that this game has a clear Nash-Equilibrium

This method is a variant of ``Generative Adversarial Networks'', first proposed by \textcite{goodfellow2014generative} (later published as \textcite{goodfellow2020generative}).
There, two neural networks take the role of generator and discriminator and instead of estimating a parameter vector, noise is transformed into some output, such as an image.
While GANs achieved great success in image generation and related tasks, %cite?
they are not directly suitable for structural estimation.
One reason is that the functional form of the generator network is usually very complex, with nodes being fully connected and activation functions being used.
Relatedly, the exact architecture of a neural network is usually not chosen to be economically (or at all) interpretable, but rather as an imprecise ``art'' based on predictive performance.
Therefore, one essential contribution of \textcite{kaji2023adversarial} is to impose that the generator has the structure of an economic model.
This model being fully specified by $\theta$ is what makes adversarial estimation meaningful.
It wouldn't be if $\theta$ were a long list of the weights and biases in a multi-layered neural network.

An implementation of \ref{eq:adversarial_estimator} looks, generally, like algorithm \ref{alg:adversarial_estimation}.

\begin{algorithm}
    \caption{Adversarial estimation}
    \label{alg:adversarial_estimation}
    \begin{algorithmic}
        \STATE Set necessary hyperparameters and initial values
        \STATE Sample real observations
        %TODO when sample noise?
        \WHILE{Stopping criterion does not hold}
            \STATE Generate fake observations $\widetilde{X}$ from the current generator
            \STATE Train the discriminator given the fake observations
            \STATE Calculate the loss
            \STATE Update $\hat{\theta}$ %the generator
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

There are various ways to fill in the details of this algorithm.
The stopping criterion might be a convergence criterion of the generator's optimization problem, or simply a sufficiently high number of repetitions being reached.
%TODO when sample noise?
The discriminator might take various forms, which I discuss below.
There are two canonical choices for the loss function, which I discuss afterwards.
The updates of the generator can be done with a gradient descent algorithm if it is differentiable or at least smooth enough that calculating numerical gradients will not lead an optimizer astray.
Otherwise, they should be performed with a gradient-free optimization procedure.

Algorithm 1 in \textcite{kaji2023adversarial} illustrates one way to fill out the details of \ref{alg:adversarial_estimation}.
They use convergence as a stopping criterion, a (not necessarily trained to completion) neural network discriminator, cross-entropy loss, and update the generator using a version of the popular Adam (\cite{diederik2014adam}).
Their simulation code shows another way. %, which I discuss in detail in section \ref{sec:simulation},
There, they compare a range of estimators (including neural networks trained to completion) and update the generator using a gradient-free approach.

Now I discuss some of these terms in detail.

\subsection{Examples of discriminators}
\label{sec:discriminators}
All the following discriminators have in common that for a data point $x$ they return a probability that it is from the real rather than the simulated data.
How this probability is then turned into an objective for the generator will be discussed in the next subsection.

Recall the unique Nash equilibrium from \dots.
If the true densities $p_0$ and $p_\theta(x)$ are known, we get the \textit{oracle discriminator}. %\dots shows that in the unique Nash equilibrium the discriminator assigns each $x$ the probability

\begin{definition}[Oracle discriminator]
    The \textbf{oracle discriminator} assigns
    \begin{equation}
        D_\theta(x):=\frac{p_0(x)}{p_0(x)+p_\theta(x)} %TODO delete theta index or copy to others?
    \end{equation}
    to every $x \in$ . %TODO    
\end{definition}

\textcite{kaji2023adversarial} call this the \textit{oracle discriminator}.
Of course, $p_0$ and $p_\theta(x)$ are unkown in practice.
Also, this discriminator is only optimal in the Nash equilibrium %todo add in proposition 1
as off-equilibrium, it neglects to update from the prior proabilities $p_0$ and $p_\theta(x)$.  %TODO maybe write more about htis
Nevertheless, it is useful as a benchmark in simulations and has an interesting theoretical property:
If the simulated sample size $m \rightarrow \infty$, $\theta_{oracle}$ approaches $\theta_{MLE}$. %(why) is this true in the general case?

A simple statistical method for classification is logistic regression.
In line with the simulation study in \textcite{kaji2023adversarial}, I consider a version that regresses on some collection of features of the data points and moments of the data.

\begin{definition}[Logistic discriminator]
    Let $\Lambda$ be a sigmoid function with values in $(0,1)$, and $x^{mom}$ an $(i+j)\times k$-matrix of features and moments of the data calculated for each data point.
    Let $(\beta_0, \dots,\beta_k \in \mathbb{R}^{k+1})$ be coefficients of a logistic regression run with $x^{mom}$ as a regressor and an output vecor $Y$ consisting of 0s and 1s for the simulated and true observations. %wording
    Then the \textbf{logistic discriminator} assigns
    \begin{equation}
        D(x) = \Lambda(\beta_{0} + \sum_{k=1}^{K}\beta_{k} x_{k}^{mom}) %TODO can I write x_k^mom like this?
    \end{equation}
    to every $x \in$ . %TODO
\end{definition}

Note that this classifier has to be calculated anew after each update of $\theta$.
While this calculation will usually be fast on modern computers, the same is not necessarily true of the potentially more powerful neural network discriminator.
Therefore, neural networks are often not trained to completion in practice and we different training procedures might result in different discriminators.

\begin{definition}[Neural network discriminator]
    Let $\mathcal{N}(\theta_{train}): \mathcal{X} \rightarrow [0, 1]$ be a classifier neural networked trained according to $\theta_{train}$.
    Then the \textbf{neural network discriminator} assigns
    \begin{equation}
        D(x) = \mathcal{N}(\theta_{train})(x)
    \end{equation}
    to every $x \in$ . %TODO
\end{definition}

To understand more deeply the loss landscape which a neural network discriminator builds for the generator, we must consider the loss function on which it is trained.

\subsection{Generator objectives}
\label{sec:losses}

\subsubsection{Jensen-Shannon divergence}
\label{sec:ce_loss}

%Following \textcite{goodfellow2014generative}, \textcite{kaji2023adversarial} choose the classical GAN objective.
The classical way to turn the probabilities $D(x)$ into an objective for the generator is the following: 

\begin{definition}[Cross-entropy loss] %TODO not a loss in the sense in which I used the term above!
    The empirical cross-entropy loss (CE) is:
    $$
    \frac{1}{n} \sum_{i=1}^n \log D\left(X_i\right)+\frac{1}{m} \sum_{i=1}^m \log \left(1-D\left(X_{i, \theta}\right)\right) \text {. }
    $$    
\end{definition}

A discriminator that maximizes the cross-entropy loss thereby calculates the Jensen-Shannon divergence (plus a constant) for the generator to minimize.
\begin{theorem}[\cite{goodfellow2014generative}]
    $\cdots$
\end{theorem}
A neural network discriminator that is not trained to completion returns an approximation of the Jensen-Shannon divergence.
In practice, only such an approximation is often used, for two reasons:
First, the training a neural network to completion multiple times for every gradient calucation of the generator's optimizer can be very computationally costly, %TODO is it also done like this with torch?
especially given that GANs in practice are often large neural networks and are applied to high-dimensional data sets.
Second, an imprecise estimate of the gradient often still leads to convergence. %TODO theory? experience?

However, even the Jensen-Shannon divergence calculated by an optimal discriminator has a crucial disadvatage:
The divergence is maximal if $p_G$ and $p_0$ have disjoint support. %TODO...are disjoint? don't overlap
Therefore, there are regions of the loss landscape where even the optimal CE-discriminator provides a gradient of zero in every direction at every point to the generator.
If the generator ``ends up'' in such a region or the initial guess is there, algorithm \ref{alg:adversarial_estimation} is unlikely to converge.
%TODO maybe sth about this being more likely in high-dimensional spaces?
Luckily, there are $\cdots$

\subsubsection{Wasserstein-p distance}
\label{sec:wasserstein_loss}

Using the so-called Wasserstein-1 distance as an optimization target for the generator was first proposed by \Textcite{arjovsky2017wassersteingan}.%TODO later published?

\begin{definition}[Wasserstein-p distance]
    For two probability distribution $\mathbb{P}_0$ and $\mathbb{P}_G$, let $\Pi (\mathbb{P}_0, \mathbb{P}_G)$ be the set of all joint distributions $\gamma(x, y)$ whose marginals are $\mathbb{P}_r$ and $\mathbb{P}_g$.
    Then for $p \geq 1$,%\in [1, +\infty]$,
    $$
    W_p(\mathbb{P}_0, \mathbb{P}_G) = \inf_{\gamma \in \Gamma(\mathbb{P}_0, \mathbb{P}_G)} \left(\mathbf{E}_{(x, y) \sim \gamma} d(x, y)^p \right)^{1/p},
    $$
    is the Wasserstein-p distance (also Wasserstein p-distance) between $\mathbb{P}_0$ and $\mathbb{P}_G$.
\end{definition}

A natural interpretation of this equation comes from the field of optimal transport.
It quantifies how much probability mass has to be moved how far in order to transfer $\mathbb{P}_0$ into $\mathbb{P}_G$, or vice versa, assuming that this transport is done optimally.
Inspired by this image, the Wasserstein-1 distance is also called \textit{Earth-Mover distance}.

The Wasserstein distances deliver a measure of the distance between two distributions that is strictly monotone even if they are non-overlapping. %TODO wording
However, since they require a solution to the optimal transport problem, they can be demanding to calculate, especially in high-dimensional spaces. %TODO true?
In the context of GANs, it is natural to consider approximating it using a neural network.
To this end, the following fact is helpful:

\begin{theorem}[Kantorovich-Rubinstein duality]
    \begin{equation}
        W_{1}\left(\mathbb{P}_0, \mathbb{P}_G\right)=\sup _{\|f\|_L \leq 1} \mathbb{E}_{x \sim \mathbb{P}_0}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_G}[f(x)]
    \end{equation}
\end{theorem}

This dual representation of the Wasserstein-1 distance paves the way to approximating it using a neural network.
The network estimates the function $f$ and is often called critic instead of discriminator since it does not return a probability anymore.
Unfortunately, it is not trivial to regularize a neural network to obey the Lipschitz constraint.
\Textcite{arjovsky2017wassersteingan} clamp the weights of the neural network to lie in a compact space.
They themselves describe this approach as ``clearly terrible'', since there is no principled way to chose the clipping parameter and setting it too big or too small comes with difficult trade-offs. %TODO rewrite?

\Textcite{gulrajani2017improvedtrainingwassersteingans} propose to penalize the norm of the gradient of the critic with respect to its input.
This solution has been more widely accepted and is also used by \textcite{athey2021using}.

%TODO how much theory? def more? or just sketch?
Of course, it is also possible to approximate the Wasserstein-1 distance without training a neural network.
Since the Wasserstein distance is the solution to an optimal transport problem, it can be derived using a ``Pseudo-auction algorithm''.
For differentiability, this algorithm can than be approximated using a smoothed ``soft-auction'' algorithm.
The divergence resulting from this algorithm is called the \textit{Sinkhorn divergence.} %TODO divergence???

\subsection{Theoretical properties}
\label{sec:theory}

\textcite{kaji2023adversarial} contains three main theoretical result.
In line with the focus of their paper, they are stated for the case with the (estimated) Jensen-Shannon distance as a criterion.

\subsubsection{Theorems in \cite{kaji2023adversarial}}
\label{sec:theorems_paper}

The first states that for some reasonable conditions on the oracle and estimated loss, the adversarial estimator is indeed consistent.

The second states that it convergeges at a rate of $O^{*}_P(n^{-1/2})$ and requires the following assumptions:
First, that the generator is parametric and well-behaved in the parameters.
The second condition, that $m$ diverges faster than $n$, can easily be guaranteed by the researcher.

The third assumption has two parts: The first is that the estimted criterion converges to its minimum for the true $\theta$ at rate $o^{*}_P(n^-1)$. %TODO make \theta_0 consistently mean true or initial value
The second part, which the authors call ``orthogonality'', %why?
is that the derivative of the estimated loss converges to that of the oracle. %TODO maybe reword
It can be empirically checked by plotting cross-sections of the loss around the original value, as the authors and I do in the simulation part.

The fourth assumption imposes two requirements to aid identification:
That the true loss has approximately quadratic curvature near the optimum and that $P_{\theta}$ and $P_{\theta_0}$ overlap.

With the conclusion of this, again requiring Assumptions 2 and 3, and a fifth assumption on the (twice) differentiability of the structural model, 
\textcite{kaji2023adversarial} arrive at their third theorem.
It states the asymptotic distribution towards which the adversarial estimator weakly converges.

In fact, assuming correct specification, a corollary states that the generator is efficient. %TODO reword?


\subsubsection{Applicability to simulations}
\label{sec:theoerem_simulation}

