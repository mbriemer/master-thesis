%\section{Method}
%\label{sec:method}

\section{Background} %TODO make section?
\label{sec:background}

\subsection{Structural estimation}
\label{sec:structural_estimation}

Consider the problem of estimating the parameters of a structural economic model.
%TODO Is this a good framing?
For $k \in \{1,\dots,K\}$, let

\begin{equation}
    Y_k = f_{\Theta}(X_k, Z_k; ),
\end{equation}

where $Y_k$ is a vector of outcome variables influenced by a vector of noise variables $Z_k$. %via a set of (possibly endogeneous) variables $X_k$.
The strength and functional form of the relationships between the variables is defined by a function $f$ and its parameters $\Theta$.

A common approach to this problem is maximum likelihood estimation, that is, to find $\hat{\theta}$ such that

\begin{equation}
    \hat{\theta} = \underset{\theta\in\Theta}{\operatorname{arg\;max}}\,\mathcal{L}_{n}(\theta\,;\mathbf{y}) ~.
\end{equation}

However, for some more sophisticated economic models, it is not easy or even possible to calculate the likelihood function.

This motivates further approaches, such as simulation methods, which attempts to infer $\theta$ based on a simulation of the true data.
Most notable among these is perhaps the simulated method of moments.

The question naturally arises of how to judge whether the simulated distribution comes sufficiently close to the real distribution.
This is one motivation for adversarial estimation.
It is also intuitive that one aproach to this involves classification.
In machine learning, popular tool for classification are neural networks, which I introduce next.

\subsection{Neural networks}
\label{sec:neural_networks}

Definition

Training

\section{Adversarial estimation}
\label{sec:adversarial_estimation}

The basic idea of adversarial estimation is to structure the parameter estimation around two auxiliary models, called the \textit{generator} and the \textit{discriminator}.
They ``play against each other'' based on a parameter estimate $\hat{\theta_{t}}$ which gets updated iteratively. 
The generator $G(\hat{\theta}) : Z \rightarrow O$ creates simulated data based on a guess of the true parameter value $\hat{\theta}$.
The discriminator $D_t : O \rightarrow [0 , 1]$ is a classifier that returns the probability of a given observation being real rather than coming from the generator.
If \text{loss} is some objective function that measures the distance between the fake and real samples, the adversarial estimator solve the problem
 
\begin{equation}
\label{eq:adversarial_estimator}
    \hat{\theta_{adv}} = \underset{\theta \in \Theta}{\arg \min } \max _{D \in \mathcal{D}_n} \text{loss}(D(X_i), D(G(\theta))).
\end{equation}

Note that this game has a clear Nash-Equilibrium

This method is a variant of ``Generative Adversarial Networks'', first proposed by \textcite{goodfellow2014generative} (later published as \textcite{goodfellow2020generative}).
There, two neural networks take the role of generator and discriminator and instead of estimating a parameter vector, noise is transformed into some output, such as an image.
While GANs achieved great success in image generation and related tasks, %cite?
they are not directly suitable for structural estimation.
One reason is that the functional form of the generator network is usually very complex, with nodes being fully connected and activation functions being used.
Relatedly, the exact architecture of a neural network is usually not chosen to be economically (or at all) interpretable, but rather as an imprecise ``art'' based on predictive performance.
Therefore, one essential contribution of \textcite{kaji2023adversarial} is to impose that the generator has the structure of an economic model.
This model being fully specified by $\theta$ is what makes adversarial estimation meaningful.
It wouldn't be if $\theta$ were a long list of the weights and biases in a multi-layered neural network.

An implementation of \ref{eq:adversarial_estimator} looks, generally, like algorithm \ref{alg:adversarial_estimation}.

\begin{algorithm}
    \caption{Adversarial estimation}
    \label{alg:adversarial_estimation}
    \begin{algorithmic}
        \STATE Set necessary hyperparameters and initial values
        \STATE Sample real observations
        %TODO when sample noise?
        \WHILE{Stopping criterion does not hold}
            \STATE Generate fake observations from the current generator
            \STATE Train the discriminator given the fake observations
            \STATE Calculate the loss
            \STATE Update $\hat{\theta}$ %the generator
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

There are various ways to fill in the details of this algorithm.
The stopping criterion might be a convergence criterion of the generator's optimization problem, or simply a sufficiently high number of repetitions being reached.
%TODO when sample noise?
The discriminator might take various forms, which I discuss below.
There are two canonical choices for the loss function, which I discuss afterwards.
The updates of the generator can be done with a gradient descent algorithm if it is differentiable or at least smooth enough that calculating numerical gradients will not lead an optimizer astray.
Otherwise, they should be performed with a gradient-free optimization procedure.

Algorithm 1 in \textcite{kaji2023adversarial} illustrates one way to fill out the details of \ref{alg:adversarial_estimation}.
They use convergence as a stopping criterion, a (not necessarily trained to completion) neural network discriminator, cross-entropy loss, and update the generator using a version of the popular Adam algorithm (\cite{diederik2014adam}), which requires setting a range of hyperparameters.
Their simulation code shows another way. %, which I discuss in detail in section \ref{sec:simulation},
There, they compare a range of estimators (including neural networks trained to completion) and update the generator using a gradient-free approach.

Now I discuss some of these terms in detail.

\subsection{Some discriminators}
\label{sec:discriminators}

Recall the unique Nash equilibrium from \dots.
If the true densities $p_0$ and $p_\theta(x)$ are known, we get the \textit{oracle discriminator}. %\dots shows that in the unique Nash equilibrium the discriminator assigns each $x$ the probability

\begin{definition}[Oracle discriminator]
    The \textbf{oracle discriminator} assigns
    \begin{equation}
        D_\theta(x):=\frac{p_0(x)}{p_0(x)+p_\theta(x)} %TODO delete theta index or copy to others?
    \end{equation}
    to every $x \in$ . %TODO    
\end{definition}

\textcite{kaji2023adversarial} call this the \textit{oracle discriminator}.
Of course, $p_0$ and $p_\theta(x)$ are unkown in practice.
Also, this discriminator is only optimal in the Nash equilibrium
as off-equilibrium, it neglects to update from the prior proabilities $p_0$ and $p_\theta(x)$.  %TODO maybe write more about htis
Nevertheless, it is useful as a benchmark in simulations and has an interesting theoretical property:
If the simulated sample size $m \rightarrow \infty$, $\theta_{oracle}$ approaches $\theta_{MLE}$. %(why) is this true in the general case?

A simple statistical method for classification is logistic regression.
Inspired by the simulation study in \textcite{kaji2023adversarial}, I consider a version that regresses on some collection of features of the data points and moments of the data.

\begin{definition}[Logistic discriminator]
    Let $\Lambda$ be a sigmoid function with values in $(0,1)$, and $x^{mom}$ an $(i+j)\times k$-matrix of features and moments of the data calculated for each data point.
    Let $(\beta_0, \dots,\beta_k \in \mathbb{R}^{k+1})$ be coefficients of a logistic regression run with $x^{mom}$ as a regressor and an output vecor $Y$ consisting of 0s and 1s for the simulated and true observations. %wording
    Then the \textbf{logistic discriminator} assigns
    \begin{equation}
        D(x) = \Lambda(\beta_{0} + \sum_{k=1}^{K}\beta_{k} x_{k}^{mom}) %TODO can I write x_k^mom like this?
    \end{equation}
    to every $x \in$ . %TODO
\end{definition}

Note that this classifier has to be calculated anew after each update of $\theta$.
While this is calculation will usually be fast on modern computers, the same is not necessarily true of the potentially more powerful neural network discriminator.

\begin{definition}[Neural network discriminator]
    Define a classifier neural network $\mathcal{N}: \mathcal{X}^k \rightarrow [0, 1]$ by: %TODO, what to call input space and general formalism
    \begin{equation} 
        \mathcal{N}(x) = \sigma_L(W_L \sigma_{L-1}(W_{L-1} \cdots \sigma_1(W_1 x + b_1) \cdots + b_{L-1}) + b_L),
    \end{equation}
    where $x \in \mathbb{R}^n$ is the input vector, $L$ is the number of layers, $W_i$ are weight matrices, $b_i$ are bias vectors, $\sigma_i$ are activation functions. %TODO rewrite
    Assume that this network has been trained at least one step on the classification problem at hand. %TODO wording
    Then the \textbf{neural network discriminator} assigns
    \begin{equation}
        D(x) = \mathcal{N}(x)
    \end{equation}
    to every $x \in$ . %TODO
\end{definition}

While neural networks are fully defined by their structure, activation functions, weights, and biases, the latter two are the result of their training.
So to implement the neural net classifier in practice, its training has to be specified, including training algorithm, hyperparameters, and number of training steps.
\dots

\subsection{Losses}
\label{sec:losses}

\subsubsection{Cross-entropy loss}
\label{sec:ce_loss}

Following \textcite{goodfellow2014generative}, \textcite{kaji2023adversarial} chose to minimize:

\begin{definition}
    The empirical cross-entropy loss:
    $$
    \frac{1}{n} \sum_{i=1}^n \log D\left(X_i\right)+\frac{1}{m} \sum_{i=1}^m \log \left(1-D\left(X_{i, \theta}\right)\right) \text {. }
    $$
    
\end{definition}


\subsubsection{Wasserstein loss}
\label{sec:wasserstein_loss}