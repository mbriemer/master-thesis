%\section{Method}
%\label{sec:method}

\section{Background} %TODO make section?
\label{sec:background}

\subsection{Structural estimation}
\label{sec:structural_estimation}

Consider the problem of estimating the parameters of a structural economic model.
%TODO Is this a good framing?
For $k \in \{1,\dots,K\}$, let

\begin{equation}
    Y_k = f_{\Theta}(X_k, Z_k; ),
\end{equation}

where $Y_k$ is a vector of outcome variables influenced by a vector of noise variables $Z_k$. %via a set of (possibly endogeneous) variables $X_k$.
The strength and functional form of the relationships between the variables is defined by a function $f$ and its parameters $\Theta$.

A common approach to this problem is maximum likelihood estimation, that is, to find $\hat{\theta}$ such that

\begin{equation}
    \hat{\theta} = \underset{\theta\in\Theta}{\operatorname{arg\;max}}\,\mathcal{L}_{n}(\theta\,;\mathbf{y}) ~.
\end{equation}

However, for some more sophisticated economic models, it is not easy or even possible to calculate the likelihood function.

This motivates further approaches, such as simulation methods, which attempts to infer $\theta$ based on a simulation of the true data.
Most notable among these is perhaps the simulated method of moments.

The question naturally arises of how to judge whether the simulated distribution comes sufficiently close to the real distribution.
This is one motivation for adversarial estimation.
It is also intuitive that one aproach to this involves classification.
In machine learning, a popular tool for classification are neural networks, which I introduce next.

\subsection{Neural networks}
\label{sec:neural_networks}

Definition

Training

\section{Adversarial estimation}
\label{sec:adversarial_estimation}

The basic idea of adversarial estimation is to structure the parameter estimation around two auxiliary models, called the \textit{generator} and the \textit{discriminator} (or \textit{critic}).
The generator $G(\hat{\theta}) : Z \rightarrow O$ creates simulated data based on a guess of the true parameter value $\hat{\theta}$.
Given the real and the simulated data, the discriminator returns objective function for the generator, which I will call \textit{loss}.
This loss function can be any divergence or distance between the distributions of the real and simulated data, inlcuding functions that are directy analytically tractable.
In the classic GAN case involving neural networks, the loss function is the result of the discriminator solving a maximization problem:

\begin{equation}
\label{eq:adversarial_estimator}
    \hat{\theta_{adv}} = \underset{\theta \in \Theta}{\arg \min } \max _{D \in \mathcal{D}} \text{loss}(D(X_i, G(\theta))).
\end{equation}

Note that this game has a clear Nash-Equilibrium

This method is a variant of ``Generative Adversarial Networks'', first proposed by \textcite{goodfellow2014generative} (later published as \textcite{goodfellow2020generative}).
There, two neural networks take the role of generator and discriminator and instead of estimating a parameter vector, noise is transformed into some output, such as an image.
While GANs achieved great success in image generation and related tasks, %cite?
they are not directly suitable for structural estimation.
One reason is that the functional form of the generator network is usually very complex, with nodes being fully connected and activation functions being used.
Relatedly, the exact architecture of a neural network is usually not chosen to be economically (or at all) interpretable, but rather as an imprecise ``art'' based on predictive performance.
Therefore, one essential contribution of \textcite{kaji2023adversarial} is to impose that the generator has the structure of an economic model.
This model being fully specified by $\theta$ is what makes adversarial estimation meaningful.
It wouldn't be if $\theta$ were a long list of the weights and biases in a multi-layered neural network.

An implementation of \ref{eq:adversarial_estimator} looks, generally, like algorithm \ref{alg:adversarial_estimation}.

\begin{algorithm}
    \caption{Adversarial estimation}
    \label{alg:adversarial_estimation}
    \begin{algorithmic}
        \STATE Set necessary hyperparameters and initial values
        \STATE Sample real observations
        %TODO when sample noise?
        \WHILE{Stopping criterion does not hold}
            \STATE Generate fake observations from the current generator
            \STATE Train the discriminator given the fake observations
            \STATE Calculate the loss
            \STATE Update $\hat{\theta}$ %the generator
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

There are various ways to fill in the details of this algorithm.
The stopping criterion might be a convergence criterion of the generator's optimization problem, or simply a sufficiently high number of repetitions being reached.
%TODO when sample noise?
The discriminator might take various forms, which I discuss below.
There are two canonical choices for the loss function, which I discuss afterwards.
The updates of the generator can be done with a gradient descent algorithm if it is differentiable or at least smooth enough that calculating numerical gradients will not lead an optimizer astray.
Otherwise, they should be performed with a gradient-free optimization procedure.

Algorithm 1 in \textcite{kaji2023adversarial} illustrates one way to fill out the details of \ref{alg:adversarial_estimation}.
They use convergence as a stopping criterion, a (not necessarily trained to completion) neural network discriminator, cross-entropy loss, and update the generator using a version of the popular Adam algorithm (\cite{diederik2014adam}), which requires setting a range of hyperparameters.
Their simulation code shows another way. %, which I discuss in detail in section \ref{sec:simulation},
There, they compare a range of estimators (including neural networks trained to completion) and update the generator using a gradient-free approach.

Now I discuss some of these terms in detail.

\subsection{Examples of discriminators}
\label{sec:discriminators}
All the following discriminators have in common that for a data point $x$ they return a probability that it is from the real rather than the simulated data.
How this probability is then turned into an objective for the generator will be discussed in the next subsection.

Recall the unique Nash equilibrium from \dots.
If the true densities $p_0$ and $p_\theta(x)$ are known, we get the \textit{oracle discriminator}. %\dots shows that in the unique Nash equilibrium the discriminator assigns each $x$ the probability

\begin{definition}[Oracle discriminator]
    The \textbf{oracle discriminator} assigns
    \begin{equation}
        D_\theta(x):=\frac{p_0(x)}{p_0(x)+p_\theta(x)} %TODO delete theta index or copy to others?
    \end{equation}
    to every $x \in$ . %TODO    
\end{definition}

\textcite{kaji2023adversarial} call this the \textit{oracle discriminator}.
Of course, $p_0$ and $p_\theta(x)$ are unkown in practice.
Also, this discriminator is only optimal in the Nash equilibrium %todo add in proposition 1
as off-equilibrium, it neglects to update from the prior proabilities $p_0$ and $p_\theta(x)$.  %TODO maybe write more about htis
Nevertheless, it is useful as a benchmark in simulations and has an interesting theoretical property:
If the simulated sample size $m \rightarrow \infty$, $\theta_{oracle}$ approaches $\theta_{MLE}$. %(why) is this true in the general case?

A simple statistical method for classification is logistic regression.
In line with the simulation study in \textcite{kaji2023adversarial}, I consider a version that regresses on some collection of features of the data points and moments of the data.

\begin{definition}[Logistic discriminator]
    Let $\Lambda$ be a sigmoid function with values in $(0,1)$, and $x^{mom}$ an $(i+j)\times k$-matrix of features and moments of the data calculated for each data point.
    Let $(\beta_0, \dots,\beta_k \in \mathbb{R}^{k+1})$ be coefficients of a logistic regression run with $x^{mom}$ as a regressor and an output vecor $Y$ consisting of 0s and 1s for the simulated and true observations. %wording
    Then the \textbf{logistic discriminator} assigns
    \begin{equation}
        D(x) = \Lambda(\beta_{0} + \sum_{k=1}^{K}\beta_{k} x_{k}^{mom}) %TODO can I write x_k^mom like this?
    \end{equation}
    to every $x \in$ . %TODO
\end{definition}

Note that this classifier has to be calculated anew after each update of $\theta$.
While this calculation will usually be fast on modern computers, the same is not necessarily true of the potentially more powerful neural network discriminator.
Therefore, neural networks are often not trained to completion in practice and we different training procedures might result in different discriminators.

\begin{definition}[Neural network discriminator]
    Define a classifier neural network $\mathcal{N}: \mathcal{X}^k \rightarrow [0, 1]$ by: %TODO, what to call input space and general formalism
    \begin{equation} 
        \mathcal{N}(x) = \sigma_L(W_L \sigma_{L-1}(W_{L-1} \cdots \sigma_1(W_1 x + b_1) \cdots + b_{L-1}) + b_L),
    \end{equation}
    where $x \in \mathbb{R}^n$ is the input vector, $L$ is the number of layers, $W_i$ are weight matrices, $b_i$ are bias vectors, $\sigma_i$ are activation functions. %TODO rewrite
    Let $\theta_{train}$ be a collection of hyperparameters that specify the training of a neural network, such as a loss function, an intial guess, a stopping condition, a training algorithm, and a list of hyperparameters of the latter. %TODO extend/rewrite?
    Call $\mathcal{N}(\theta_{train})$ the network $\mathcal{N}$ after it has been trained according to $\theta_{train}$.
    Then the \textbf{neural network discriminator} assigns
    \begin{equation}
        D(x) = \mathcal{N}(\theta_{train})(x)
    \end{equation}
    to every $x \in$ . %TODO
\end{definition}

To understand more deeply the loss landscape which a neural network discriminator builds for the generator, we must consider the loss function on which it is trained.

\subsection{Generator objectives}
\label{sec:losses}

\subsubsection{Jensen-Shannon divergence}
\label{sec:ce_loss}

%Following \textcite{goodfellow2014generative}, \textcite{kaji2023adversarial} choose the classical GAN objective.
The classical way to turn the proabilities $D(x)$ into an objective for the generator is the following: 

\begin{definition}[Cross-entropy loss] %TODO not a loss in the sense in which I used the term above!
    The empirical cross-entropy loss (CE) is:
    $$
    \frac{1}{n} \sum_{i=1}^n \log D\left(X_i\right)+\frac{1}{m} \sum_{i=1}^m \log \left(1-D\left(X_{i, \theta}\right)\right) \text {. }
    $$    
\end{definition}

A discriminator that maximizes the cross-entropy loss thereby calculates the Jensen-Shannon divergence (plus a constant) for the generator to minimize.
\begin{theorem}[\cite{goodfellow2014generative}]
    $\cdots$
\end{theorem}
A neural network discriminator that is not trained to completion returns an approximation of the Jensen-Shannon divergence.
In practice, only such an approximation is often used, for two reasons:
First, the training a neural network to completion multiple times for every gradient calucation of the generator's optimizer can be very computationally costly, %TODO is it also done like this with torch?
especially given that GANs in practice are often large neural networks and are applied to high-dimensional data sets.
Second, an imprecise estimate of the gradient often still leads to convergence. %TODO theory? experience?

However, even the Jensen-Shannon divergence calculated by an optimal discriminator has a crucial disadvatage:
The divergence is maximal if $p_G$ and $p_0$ have disjoint support. %TODO...are disjoint? don't overlap
Therefore, there are regions of the loss landscape where even the optimal CE-discriminator provides a gradient of zero in every direction at every point to the generator.
If the generator ``ends up'' in such a region or the initial guess is there, algorithm \ref{alg:adversarial_estimation} is unlikely to converge.
%TODO maybe sth about this being more likely in high-dimensional spaces?
Luckily, there are $\cdots$

\subsubsection{Wasserstein-p distance}
\label{sec:wasserstein_loss}

Using the so-called Wasserstein-1 distance as an optimization target for the generator was first proposed by \Textcite{arjovsky2017wassersteingan}.%TODO later published?

\begin{definition}[Wasserstein-p distance]
    For two probability distribution $\mathbb{P}_0$ and $\mathbb{P}_G$, let $\Pi (\mathbb{P}_0, \mathbb{P}_G)$ be the set of all joint distributions $\gamma(x, y)$ whose marginals are $\mathbb{P}_r$ and $\mathbb{P}_g$.
    Then for $p \geq 1$,%\in [1, +\infty]$,
    $$
    W_p(\mathbb{P}_0, \mathbb{P}_G) = \inf_{\gamma \in \Gamma(\mathbb{P}_0, \mathbb{P}_G)} \left(\mathbf{E}_{(x, y) \sim \gamma} d(x, y)^p \right)^{1/p},
    $$
    is the Wasserstein-p distance (also Wasserstein p-distance) between $\mathbb{P}_0$ and $\mathbb{P}_G$.
\end{definition}

A natural interpretation of this equation comes from the field of optimal transport.
It quantifies how much probability mass has to be moved how far in order to transfer $\mathbb{P}_0$ into $\mathbb{P}_G$, or vice versa, assuming that this transport is done optimally.
Inspired by this image, the Wasserstein-1 distance is also called \textit{Earth-Mover distance}.

The Wasserstein distances deliver a measure of the distance between two distributions that is strictly monotone even if they are non-overlapping. %TODO wording
However, since they require a solution to the optimal transport problem, they can be demanding to calculate, especially in high-dimensional spaces. %TODO true?
In the context of GANs, it is natural to consider approximating it using a neural network.
To this end, the following fact is helpful:

\begin{theorem}[Kantorovich-Rubinstein duality]
    \begin{equation}
        W_{1}\left(\mathbb{P}_0, \mathbb{P}_G\right)=\sup _{\|f\|_L \leq 1} \mathbb{E}_{x \sim \mathbb{P}_0}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_G}[f(x)]
    \end{equation}
\end{theorem}

This dual representation of the Wasserstein-1 distance paves the way to approximating it using a neural network.
The network estimates the function $f$ and is often called critic instead of discriminator since it does not return a probability anymore.
Unfortunately, it is not trivial to regularize a neural network to obey the Lipschitz constraint.
\Textcite{arjovsky2017wassersteingan} clamp the weights of the neural network to lie in a compact space.
They themselves describe this approach as ``clearly terrible'', since there is no principled way to chose the clipping parameter and setting it too big or too small comes with difficult trade-offs. %TODO rewrite?

\Textcite{gulrajani2017improvedtrainingwassersteingans} propose to penalize the norm of the gradient of the critic with respect to its input.
This solution has been more widely accepted and is also used by \textcite{athey2021using}.

%TODO how much theory? def more? or just sketch?
Of course, it is also possible to approximate the Wasserstein-1 distance without training a neural network.
Since the Wasserstein distance is the solution to an optimal transport problem, it can be derived using a ``Pseudo-auction algorithm''.
For differentiability, this algorithm can than be approximated using a smoothed ``soft-auction'' algorithm.
The divergence resulting from this algorithm is called the \textit{Sinkhorn divergence.} %TODO divergence???

\subsection{Consistency and convergence}
\label{sec:consistency_and_convergence}

\textcite{kaji2023adversarial} contains three main theoretical result.
In line with the focus of their paper, they are stated for the case with the (estimated) Jensen-Shannon distance as a criterion.

\subsubsection{Theorems in \cite{kaji2023adversarial}}
\label{sec:theorems_paper}

The first states that for some reasonable conditions on the oracle and estimated loss, the adversarial estimator is indeed consistent.

The second states that it convergeges at a rate of $O^{*}_P(n^{-1/2})$ and requires the following assumptions:
First, that the generator is parametric and well-behaved in the parameters.
The second condition, that $m$ diverges faster than $n$, can easily be guaranteed by the researcher.

The third assumption has two parts: The first is that the estimted criterion converges to its minimum for the true $\theta$ at rate $o^{*}_P(n^-1)$. %TODO make \theta_0 consistently mean true or initial value
The second part, which the authors call ``orthogonality'', %why?
is that the derivative of the estimated loss converges to that of the oracle. %TODO maybe reword
It can be empirically checked by plotting cross-sections of the loss around the original value, as the authors and I do in the simulation part.

The fourth assumption imposes two requirements to aid identification:
That the true loss has approximately quadratic curvature near the optimum and that $P_{\theta}$ and $P_{\theta_0}$ overlap.

With the conclusion of this, again requiring Assumptions 2 and 3, and a fifth assumption on the (twice) differentiability of the structural model, 
\textcite{kaji2023adversarial} arrive at their third theorem.
It states the asymptotic distribution towards which the adversarial estimator weakly converges.

In fact, assuming correct specification, a corollary states that the generator is efficient. %TODO reword?


\subsubsection{Applicability to simulations}
\label{sec:theoerem_simulation}

